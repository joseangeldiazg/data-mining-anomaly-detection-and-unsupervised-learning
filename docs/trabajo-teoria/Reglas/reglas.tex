%---------------------------------------------------
% Nombre: reglas.tex  
% 
% Texto del capitulo 1
%---------------------------------------------------

\chapter{Reglas de Asociación}

Las reglas de asociación han sido una de las técnicas más estudiadas en el campo de la minería de datos. En este capítulo, se verá el concepto de regla de asociación y su trasfondo (sección \ref{intro}), las medidas clásicas para su validación (sección \ref{validacion}), algunos de los principales algoritmos (sección \ref{algoritmos}) y finalizaremos estudiando algunas de sus aplicaciones (sección \ref{app}). 

\section{Introducción}
\label{intro}

La reglas de asociación dentro del ámbito de la informática no son muy distintas, al menos en el concepto general, de la búsqueda de relaciones en cualquier ámbito. Las reglas de asociación se enmarcan dentro del aprendizaje automático o minería de datos y no es algo nuevo sino que llevan siendo usadas y estudiadas desde mucho tiempo atrás, datando una de las primeras referencias a estas, del año 1993 \cite{agrawal}. Su utilidad es la de obtener conocimiento relevante de grandes bases de datos y se representan según la forma \textbf{X->Y} donde \textbf{X}, es un conjunto de ítems que representa el antecedente e \textbf{Y} un ítem consecuente, por ende, podemos concluir que los ítems \textbf{consecuentes}  guardan una relación de co-ocurrencia con los ítems \textbf{antecedentes}. Esta relación puede ser obvia en algunos casos, pero en otros necesitará del uso de algoritmos de extracción de reglas de asociación que podrán desvelar relaciones no triviales y que puedan ser de mucho valor.  Podremos presentar por tanto a las reglas de asociación, como un método de extracción de relaciones aparentemente ocultas entre ítems o elementos dentro de bases de datos transaccionales, \textit{datawarehouses} u otros tipos de almacenes de datos de los que es interesante extraer información de ayuda en el proceso de toma de decisiones de las organizaciones. 

\section{Validación}
\label{validacion}

La forma clásica de medir la bondad o ajuste de las reglas de asociación a un determinado problema, vendrá dada por las medidas del soporte, la confianza y el lift, que podremos definir de la siguiente manera:
\begin{itemize}

	\item Soporte: Se representa como \textit{supp (X$\rightarrow$Y)}, y representa la fracción de las transacciones que contiene tanto a X como a Y respecto al total de transacciones. Quedaría definido por la siguiente ecuación:
	\begin{equation}
	supp(X \rightarrow Y) = \frac{supp(X \cup Y)}{total transacciones}
	\end{equation}
	\item Confianza: Se representa como \textit{conf (X$\rightarrow$Y)}, y representa la fracción de transacciones en las que aparece el ítem Y, de entre aquellas transacciones donde aparece el ítem X. Su ecuación sería:
	\begin{equation}
	conf(X \rightarrow Y) = \frac{supp(X \rightarrow Y)}{supp(X)}
	\end{equation}
	\item Lift: El \textit{lift}, es una medida útil para evaluar la independencia entre los ítems de una determinada regla  de asociación. En una regla del tipo \textit{conf (X$\rightarrow$Y)}, esta medida representa el grado en que X tiende a ser frecuente cuando A está presente en la regla, o viceversa. El lift, quedará definido matemáticamente de la siguiente manera:
	\begin{equation}
	lift(X \rightarrow Y) = \frac{conf(X \rightarrow Y)}{supp(Y)}
	\end{equation}
\end{itemize}

Pese a que estas medidas son las más comunes y extendidas, hay innumerables propuestas de medias complementarias en la literatura, tales como la \textbf{convicción, factor de certeza, diferencia absoluta de confianza} entre otras muchas (figura \ref{medidas}).

\begin{figure}[H]
\centering
\includegraphics[width=13cm]{./Reglas/imagenes/medidas.jpg}
\caption{Distintas medidas aplicables a reglas de asociación.}
\label{medidas}
\end{figure} 

\section{Obtención de reglas}

Si nos centramos en la manera de obtener las reglas, estas pueden abordarse desde dos perspectivas, solución por fuerza bruta (prohibitivo) o desde un enfoque basado en dos etapas. La primera de estas etapas es la generación de itemsets frecuentes, a partir de los cuales, en la segunda etapa se obtienen las reglas de asociación, que tendrán si todo ha ido correctamente un valor de confianza aceptable o elevado. La primera etapa de obtención de itemsets frecuentes puede conllevar problemas de memoria ya que en una base de datos con muchos ítems o transacciones el número de estos será muy elevado, es por ello que surgen aproximaciones en el proceso de representación de itemsets frecuentes que nos permitirán obtener estos en bases de datos de gran tamaño. Estas aproximaciones son:

\begin{itemize}
\item Itemsets maximales: Son aquellos itemsets frecuentes para los que ninguno de los superconjuntos inmediatos al itemsets en cuestión, son frecuentes. A partir de estos podremos recuperar todos los itemsets frecuentes de manera sencilla sin tener que mantenerlos todos en memoria. 
\item Itemsets cerrados: Son aquellos itemsets frecuentes para los que ninguno de los superconjuntos inmediatos al itemsets en cuestión, tienen un soporte igual. Con esta aproximación, tendremos soportes e itemsets frecuentes que podremos recuperar fácilmente, aunque al ser más numerosos que los maximales mantenerlos en memoria puede llegar a ser complicado.  
\end{itemize}

En resumen usaremos itemsets cerrados cuando la eficiencia sea un factor a tener en cuenta o prohibitivo, frente al tamaño de la base de datos. Si estuviéramos en el caso contrario, los itemsets maximales serán nuestra opción ganadora al ser más compactos. Sea como sea, una vez obtenidos los itemsets frecuentes podemos centrarnos en la obtención de las reglas para ello, se crean tordas las posibles combinaciones de regla con el itemset y se seleccionan solo aquellas que superen el umbral de confianza definido por el experto del problema en cuestión. 

\section{Principales algoritmos}
\label{algoritmos}
En esta sección veremos una introducción a los principales algoritmos empleados en problemas de obtención de reglas de asociación.

\subsection{Apriori}

El algoritmo \textbf{Apriori}, fue propuesto por Agrawal y Srikant en 1994 \cite{apriori} y desde entonces sigue siendo el algoritmo más extendido para la obtención de itemsets frecuentes, con los que construiremos en una segunda etapa las reglas de asociación. Se basa en el principio de que si un itemset es frecuente, entonces todos sus subconjuntos también lo son por lo que al encontrar uno de estos, podremos podar el árbol de búsqueda evitando hacer comprobaciones y aumentando la eficiencia. Para obtener los itemsets frecuentes, el algoritmo en base a un valor mínimo de soporte fijado por el experto en la materia, generará todas las posibles combinaciones de itemsets y comprobará si son o no frecuentes. En cada iteración, se generan todos los posibles itemsets distintos que se pueden formar combinando los de la anterior, por lo que los itemsets irán creciendo de tamaño.

Apriori tiene bastantes factores o limitaciones relacionados con la eficiencia del algoritmo y que pueden afectar en gran medida al proceso de minería de datos que en algunos problemas específicos podría incluso resultar prohibitivo por tiempos o espacio. Algunas de estas limitaciones serían:

\begin{enumerate}
\item Soporte: Umbrales demasiado bajos conllevarán a una explosión del número de itemsets frecuentes lo que está directamente relacionado con una mayor necesidad de memoria y tiempo. 
\item Número de ítems distintos: Esta limitación, está ligada a la necesidad del algoritmo apriori de almacenar el soporte de cada uno de éstos, lo que puede conllevar problemas de memoria. 
\item Tamaño de la base de datos: Este punto está ligado, al anterior, pero en lugar de tener en cuenta los ítems individuales se tienen en cuenta el número de transacciones. Apriori al ser exhaustivo realiza múltiples pasadas por toda la base de datos por lo que el tiempo de ejecución puede ser muy elevado o incluso no llegar a acabar en varios días o semanas. 
\item Longitud de las transacciones: Ligado al problema anterior, si las transacciones a su vez están formadas por muchos ítems, almacenar esto en memoria puede llegar a ser privativo e incluso imposible. 
\end{enumerate}


\subsection{Eclat}
El algoritmo Eclat \cite{eclat}, se basa en una estructura de datos denominada tid-list, que será generada para cada ítem en la base de datos y que almacena los id de las distintas transacciones de la base de datos que contienen al ítem en cuestión. Este enfoque nos permite obtener el soporte de un k-ítemset de manera muy rápida realizando la intersección de sus subconjuntos. Por otro lado, mantener estas estructuras en memoria, puede llegar a ser imposible si la base de datos contiene muchas transacciones. 

Las limitaciones de los algoritmos tradicionales han llevado a el estudio de otros método menos sensibles a los requisitos temporales o de espacio, de cara a poder aplicar estas técnicas a mayores cantidades de datos aún. Este método es el algoritmo FP-Growth y lo estudiaremos en el siguiente punto.

\subsection{FP-Growth}

El algoritmo \textbf{FP-Growth} \cite{fpg} fue propuesto en el año 2000, como una solución a los problemas de memoria generados por los métodos típicos como el Apriori, visto anteriormente. Es un algoritmo muy eficiente y ampliamente extendido en problemas y soluciones que podrían ser enmarcados bajo el nombre de Big Data. 

\textbf{FP-Growth}, crea un modelo comprimido de la base datos original utilizando una estructura de datos que denomina como \textbf{\textit{FP-tree}} que está formada por dos elementos esenciales:

\begin{itemize}
\item Grafo de transacciones: Gracias a este grafo la base de datos completa puede abreviarse. En cada nodo, se describe un itemsets y su soporte que se calcula siguiendo el camino que va desde la raíz hasta el nodo en cuestión.
\item Tabla cabecera: Es una tabla de listas de ítems. Es decir, para cada ítem, se crea una lista que enlaza nodos del grafo donde aparece. 
\end{itemize}

Una vez se construye el árbol, utilizando un enfoque recursivo basado en divide y vencerás, se extraen los itemsets frecuentes. Para ello primero se obtienen el soporte de cada uno de los ítems que aparecen en la tabla de cabecera, tras lo cual, para cada uno de los ítems que superan el soporte mínimo se realizan los siguientes pasos:

\begin{enumerate}
\item Se extrae la sección del árbol donde aparece el ítem reajustando los valores de soporte de los ítems que aparecen en esa sección.
\item Considerando esa sección extraída, se crea un nuevo \textbf{\textit{FP-tree}}.
\item Se extraen los itemsets que superen el mínimo soporte de este último \textbf{\textit{FP-tree}} creado. 
\end{enumerate}

En función a lo estudiado, es obvio ver que la memoria que ocupa es mucho menor que la generada por Apriori, así como al generar itemsets por medio del principio divide y vencerás, \textbf{FP-Growth} se presta a ser usado en entornos distribuidos como por ejemplo el entorno de Big Data, Apache Spark, aumentando sus prestaciones de manera notable. 

\section{Aspectos Avanzados}

Las reglas ed asociación, como casi la práctica totalidad de las tecnologías  o técnicas informáticas, están en constante evolución, de ahí que se propongan nuevas variaciones de la técnica con el fin de 

\subsection{Interpretación}

\subsection{Reglas de asociación difusas}

\subsection{Análisis de reglas por grupos}

Pese a que el análisis de reglas de asociación se suele realizar de manera individual, hay diversos enfoques que proponen que un análisis en conjunto de las reglas dirigido en función de algun criterio o medidas

\section{Aplicaciones}
\label{app}

Las reglas de asociación son muy conocidas por sus aplicaciones en problemas como, el del análisis de la `cesta de la compra'. Si bien, es verdad que esta puede ser su aplicación más extendida, las reglas de asociación tienen infinitud de aplicaciones en campos tan dispares como la obtención de información a partir de los datos recopilados por aerogeneradores, datos bancarios o logísticos. Dentro de la propia ciencia de datos, las reglas de asociación pueden usarse para extender otras vertientes, como el de la minería social o la minería de textos donde se usan para asociar la presencia de términos en ciertos documentos. 

Al igual que se hizo en los capítulos anteriores de clustering  y detección de anomalías, se han recopilado algunos artículos científicos recientes que tratan sobre reglas de asociación, para ilustrar ejemplos reales de aplicación de las mismas. El primer estudio es propuesto por Hu y Guo \cite{pollutants}, donde usan un enfoque basado en reglas de asociación para la obtención de información relevante sobre el estado de la polución en el centro urbano de la ciudad de Lan-Xi-Yin. El segundo trabajo, es propuesto por Zhong \cite{basket} y está enfocado al ámbito del deporte donde por medio del algoritmo apriori, se analizan técnicas y tácticas en el baloncesto. 


\clearpage
%---------------------------------------------------